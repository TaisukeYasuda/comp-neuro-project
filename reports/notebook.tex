\documentclass{article}
\usepackage{tai}

\title{Research Notebook}
\author{Taisuke Yasuda}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{May 20, 2017}
\subsection{Progress}
\begin{itemize}
  \item set up github for project
  \item plotted histograms of first trials
  \item plotted scatter plots of first trials (for stationarity)
\end{itemize}

\subsection{Statistical Model}
Recall that our model of the process is
\[
  X = \sum_{j=1}^N Z_j,\qquad Y_j = \Bernoulli(p_j),\qquad (Z_j\mid Y_j = 1)\sim N(\mu_j,\sigma_j^2), \qquad (Z_j\mid Y_j = 0) = 0
\]
where $N, \mu_j, \sigma_j^2, p_j$ are all unknown parameters of the model. The $Z_j$ random variable models the response amplitude of a single contact of which there are $N$, and the $Y_j$ random variable models the release success of a single contact. We assume that all of the $Z_j$ and the $Y_j$ are independent.

\subsubsection{Justification}
The additivity of the potential is justified by Petterson and Einevoll \cite{pettersen2008amplitude}. The use of a Gaussian distribution for individual contacts is justified by Magee and Cook \cite{magee2000somatic}.

\section{May 22, 2017}
\subsection{Progress}
\begin{itemize}
  \item derived point estimates for release probability, mean response amplitude, and response amplitude variance under the constant parameter models
\end{itemize}

\subsection{Point Estimation of Model Parameters}
\subsubsection{Estimations Under a Simplified Model}
We have that the expectation of the model is
\[
  \mathbb E[X] = \sum_{j=1}^N \mathbb E[Z_j] = \sum_{j=1}^N \mu_j\cdot p_j
\]
and that the variance is
\[
  \Var[X] = \sum_{j=1}^N \Var[Z_j] = \sum_{j=1}^N \sigma_j^2\cdot p_j.
\]
Also note that the failure rate of the model, i.e.\ the probability that all $N$ contacts fail to release, can be approximated by
\[
  \mathbb P[X = 0]\approx \prod_{j=1}^N (1-p_j)
\]
by assuming that a contact produces a positive response everytime it succeeds in releasing a vesicle. Thus, with simplifying assumptions that all the $\mu_j$, $\sigma_j^2$, and $p_j$ are the same across the $N$ points of contact, and by fixing a value of $N$, we may find a plugin estimator for $\hat p$ and method of moments estimators for $\hat\mu$ and $\hat\sigma^2$ that depend on $\hat p$. If we let $\overline X$ be the sample mean, $S^2$ be the sample variance, and $p_f$ be the sample failure rate, these estimates are given by
\[
  \hat p = 1 - \sqrt[N]{p_f}, \qquad \hat\mu = \frac{\overline X}{N\hat p}, \qquad \hat\sigma^2 = \frac{S^2}{N\hat p}.
\]

\subsubsection{EM Algorithm}
Let us return to the general case. Suppose that we treat the response amplitudes of the individual contacts, the $Z_j$ from $1\leq j\leq N$, as latent variables. Then, the joint distribution of the latent and observed variables will be an exponential family, so EM algorithm should work very well.

\section{May 24, 2017}
\subsection{Goals}
\begin{itemize}
  \item how was the data collected? can we really justify our model?
  \item workout details of EM and implement
\end{itemize}

\subsection{Progress}
\begin{itemize}
  \item derive E step of the EM algorithm
\end{itemize}

\subsection{EM Algorithm}
We refer to lecture notes by Andrew Ng \cite{ng2016em} for the EM reference. Suppose that we fix $N$ and let $(x_i)_{i=1}^n$ be our observations of our model, let $y = (y_j)_{j=1}^N$ be the hidden observed values of the release success of each individual contact, and let $(\mu, \sigma^2, p) = (\mu_j,\sigma_j^2,p_j)_{j=1}^N$. Then, the log-likelihood of the parameters is given by
\begin{align*}
  \ell\left(\mu, \sigma^2, p\right) &= \sum_{i=1}^n\log p(x_i; \mu, \sigma^2, p) = \sum_{i=1}^n\log \sum_{y\in\{0,1\}^N} p(x_i\mid y;\mu,\sigma^2,p)\cdot p(y;\mu,\sigma^2,p).
\end{align*}
In the above, the sum ranges over all possible assignments of the release success of the $N$ individual contacts. Note that $p(z_i\mid y;\mu,\sigma^2,p)$ is simply the pdf of the sum of Gaussian variables which also takes a Gaussian distribution, and $p(y;\mu,\sigma^2,p)$ is simply given by the product of $p_j$ or $1-p_j$ for each $1\leq j\leq N$, as appropriate. Also note that this problem is very similar to a Gaussian mixture model, where we sample from $2^N$ different Gaussian distributions where each Gaussian is a sum of the original $N$ Gaussians. The difference is that the Gaussians that we find may not be arbitrary, but in fact must be generated by $N$ Gaussians in a specific way.

\subsubsection{E step}

In the E step, we find a probability distribution over the release success of the individual contacts, given the observed amplitude $x_i$. Denote $\varphi(x;\mu,\sigma^2)$ as the pdf of a Gaussian with mean $\mu$ and variance $\sigma^2$ evaluated at the point $x$. Then,
\[
  Q_i(y) = p(y\mid x_i; \mu, \sigma^2, p) = \frac{p(x_i\mid y; \mu,\sigma^2,p)\cdot p(y;\mu,\sigma^2,p)}{\sum_{z\in\{0,1\}^N} p(x_i\mid z;\mu,\sigma^2,p)\cdot p(z;\mu,\sigma^2,p)}
\]
where for all assignments of release successes $z\in\{0,1\}^N$,
\[
  p(x_i\mid z; \mu,\sigma^2,p) = \varphi\left(x_i;\sum_{j:z_j=1} \mu_j, \sum_{j:z_j=1} \sigma_j^2\right),\qquad p(z;\mu,\sigma^2,p) = \prod_{j:z_j=1}p_j\prod_{j:z_j=0}(1-p_j).
\]

\section{May 27, 2017}
\subsection{Progress}
\begin{itemize}
  \item derive M step of the EM algorithm for $p_l$
\end{itemize}

\subsection{EM Algorithm (cont.)}

\subsubsection{M step}

In the M step, we maximize the function
\[
  \ell(\theta) = \sum_{i=1}^n \sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\log \frac{p(x_i, z;\theta)}{Q_i^{(t)}(z)}
\]
and set the value of $\theta$ that maximizes the above to the new estimate for $\theta$, where $\theta$ is our $\mu,\sigma^2,p$. For notational convenience, let $\mu_z := \sum_{j:z_j=1}\mu_j$, $\sigma_z^2 := \sum_{j:z_j=1} \sigma_j^2$, and $p_z := \prod_{j:z_j=1}p_j\prod_{j:z_j=0}(1-p_j)$.
\begin{itemize}
  \item \textbf{Maximization with respect to $\mu_l$}

  We have that
  \begin{align*}
    \frac{\partial}{\partial\mu_l}\ell(\theta) &= \frac{\partial}{\partial\mu_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\log \frac{p(x_i, z;\theta)}{Q_i^{(t)}(z)}
    = \frac{\partial}{\partial\mu_l}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\log p(x_i, z;\theta) - \log Q_i^{(t)}(z)\right) \\
    &= \frac{\partial}{\partial\mu_l}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\log \left(\frac1{\sqrt{2\pi\sigma_z^2}}e^{-\frac{(x_i-\mu_z)^2}{2\sigma_z^2}}\cdot p_z\right) - \log Q_i^{(t)}(z)\right) \\
    &= \frac{\partial}{\partial\mu_l}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\log \frac1{\sqrt{2\pi\sigma_z^2}} -\frac{(x_i-\mu_z)^2}{2\sigma_z^2} + \log p_z - \log Q_i^{(t)}(z)\right) \\
    &= -\frac{\partial}{\partial\mu_l}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\frac{(x_i-\mu_z)^2}{2\sigma_z^2}
    = -\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\frac{-2(x_i-\mu_z)}{2\sigma_z^2} \\
    &= \sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\frac{x_i-\mu_z}{\sigma_z^2}.
  \end{align*}

  We cannot isolate $\mu_l$, so we need to solve with the other $\mu_j$ as well as the $\sigma_j^2$.

  \item \textbf{Maximization with respect to $\sigma_l^2$}

  We have that
  \begin{align*}
    \frac{\partial}{\partial\sigma_l^2}\ell(\theta) &= \frac{\partial}{\partial\sigma_l^2}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\log \frac{p(x_i, z;\theta)}{Q_i^{(t)}(z)} \\
    &= \frac{\partial}{\partial\sigma_l^2}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\log \frac1{\sqrt{2\pi\sigma_z^2}} -\frac{(x_i-\mu_z)^2}{2\sigma_z^2} + \log p_z - \log Q_i^{(t)}(z)\right) \\
    &= \frac{\partial}{\partial\sigma_l^2}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\log\frac1{\sqrt{2\pi\sigma_z^2}} - \frac{(x_i-\mu_z)^2}{2\sigma_z^2}\right) \\
    &= \frac{\partial}{\partial\sigma_l^2}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(-\frac{\log(2\pi\sigma_z^2)}2 - \frac{(x_i-\mu_z)^2}{2\sigma_z^2}\right)
    = \frac12\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\left(\frac{x_i-\mu_z}{\sigma_z^2}\right)^2-\frac1{\sigma_z^2}\right).
  \end{align*}

  Now we need to solve for all the $\mu_j$ and $\sigma_j$ when the above is set to $0$. It is worth noting that the $Z$-score with respect to $N(\mu_z,\sigma_z^2)$ appears in both of the derivatives for $\mu_l$ and $\sigma_l^2$.

  \item \textbf{Maximization with respect to $p_l$}

  As before, we have that
  \begin{align*}
    \frac{\partial}{\partial p_l}\ell(\theta) &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\log \frac{p(x_i, z;\theta)}{Q_i^{(t)}(z)} \\
    &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\left(\log \frac1{\sqrt{2\pi\sigma_z^2}} -\frac{(x_i-\mu_z)^2}{2\sigma_z^2} + \log p_z - \log Q_i^{(t)}(z)\right) \\
    &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\log p_z \\
    &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\left(\sum_{j:z_j=1}\log p_j + \sum_{j:z_j=0}\log(1-p_j)\right) \\
    &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\left(\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\log p_l + \sum_{\substack{z\in\{0,1\}^N \\ z_l = 0}} Q_i^{(t)}(z)\log(1-p_l)\right) \\
    &= \sum_{i=1}^n\left(\frac{\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}}Q_i^{(t)}(z)}{p_l} - \frac{\sum_{\substack{z\in\{0,1\}^N \\ z_l = 0}}Q_i^{(t)}(z)}{1-p_l}\right).
  \end{align*}
  Setting this to $0$ gives us that
  \[
    0 = \sum_{i=1}^n\left(\frac{\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}}Q_i^{(t)}(z)}{p_l} - \frac{\sum_{\substack{z\in\{0,1\}^N \\ z_l = 0}}Q_i^{(t)}(z)}{1-p_l}\right),
  \]
  which solves to
  \[
    p_l = \frac{\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}}Q_i^{(t)}(z)}{\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}}Q_i^{(t)}(z) + \sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 0}}Q_i^{(t)}(z)} = \boxed{\frac{\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}}Q_i^{(t)}(z)}n}.
  \]
\end{itemize}

\section{May 28, 2017}

\subsection{Progress}
\begin{itemize}
  \item gave up on a closed form for the M step, decided on gradient descent for the EM algorithm
\end{itemize}

\subsection{EM Algorithm (cont.)}
\subsubsection{M step (cont.)}
Since we cannot solve for the optimal $\mu_l$ and $\sigma_l^2$ in closed form, we opt for numerically determining the $\mu_l$ and $\sigma_l^2$ via gradient descent \cite{bishop2006pattern}. Recall that we have already computed the gradient:
\[
  \begin{cases}
    \frac{\partial}{\partial\mu_l}\ell(\theta) = \sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\frac{x_i-\mu_z}{\sigma_z^2} \\
    \frac{\partial}{\partial\sigma_l^2}\ell(\theta) = \frac12\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\left(\frac{x_i-\mu_z}{\sigma_z^2}\right)^2-\frac1{\sigma_z^2}\right)
  \end{cases}.
\]
Now, it is straightforward to implement the algorithm.

\subsection{Parametric Bootstrap}
Once we have an estimator for the parameters of the model, we may use parametric bootstrap to estimate the variance of the estimator, as explained in the classical text by Casella and Berger \cite{casella2002statistical}. Using the parameters $\hat\theta$ estimated by the EM algorithm, we may simulate $n$ values $X_1^*, \dots, X_n^*\sim f(x;\hat\theta)$ and approximate the MLE using the EM algorithm $B$ times, and use the sample variance of those $B$ estimates of the MLE as the variance of the estimator.

\section{May 29, 2017}
\subsection{Progress}
\begin{itemize}
  \item implement EM algorithm upto expectation step
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item validate $p_j$ estimates with the sample failure rate
  \item validate $\mu_j$ estimates with the sample mean
  \item validate $\sigma_j^2$ estimates with the sample variance
  \item after writing slower version of EM algorithm, write a faster version
\end{itemize}

\section{May 30, 2017}
\subsection{Progress}
\begin{itemize}
  \item derive EM algorithm steps for ``binomial distribution'' model
  \item implement draft of EM algorithm for ``binomial distribution'' model
  \item test EM algorithm, realize that we need to handle the $z = (0,0,\dots,0)$ case separately
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item test EM algorithm with data simulated from the generative model
  \item justify linearity of postsynaptic integration of signals
  \item investigate emergent properties
  \item separate out nonzero responses from the zero responses
  \item show that the classical binomial model does not sufficiently explain the data (figures, graphics)
  \item investigate results of Turner and West and see if they apply
  \item also try model that keeps $\sigma^2$ constant, and derive the EM algorithm for it
  \item study Kolmogorov-Smirnov test for testing the equality of the distributions
\end{itemize}

\subsection{Previous Model of EPSP Amplitudes - Binomial Distribution}
The previous models of postsynaptic potential amplitudes are usually binomial models \cite{kuhnt1992statistical}. We would like to first show that this model does not fit our data in a satisfactory way. This model assumes that the probabilities of all of the $N$ contacts are the same and that the mean response is also all the same. Let us further suppose that the variances of the Gaussians at each contact are the same and see if we may reject this hypothesis. The insufficiency of this model is also suggested by Turner and West \cite{turner1993bayesian}, and we wish to support this view.

\subsubsection{Statistical Model}
With $N$ contacts, the model is given by
\[
  X = \sum_{j=1}^N Z_j,\qquad Y_j = \Bernoulli(p),\qquad (Z_j\mid Y_j = 1)\sim N(\mu,\sigma^2), \qquad (Z_j\mid Y_j = 0) = 0
\]
with parameters $\mu, \sigma^2, p$.

\subsubsection{Method of Moments Estimator}
Recall that
\[
  \mathbb E[X] = \sum_{j=1}^N \mathbb E[Z_j] = Np\mu, \qquad \Var[X] = \sum_{j=1}^N \Var[Z_j] = Np\sigma^2.
\]
Then if we determine $p$ via the failure rate $p_f$, we find estimates
\[
  \hat p = 1 - \sqrt[N]{p_f}, \qquad \hat\mu = \frac{\overline X}{N\hat p}, \qquad \hat\sigma^2 = \frac{S^2}{N\hat p},
\]
as before.

\subsubsection{Maximum Likelihood Estimator}
The log likelihood of the above model as a function of the parameters is given by
\[
  \ell(\mu,\sigma^2,p) = \sum_{i=1}^n p(x_i;\mu,\sigma^2,p) = \sum_{i=1}^n \log \sum_{z\in\{0,1\}^N} p(x_i,z;\mu,\sigma^2,p).
\]
As above, we must employ the EM algorithm to estimate the MLE. Under these assumptions, the E step simplifies to
\[
  Q_i(y) = p(y\mid x_i; \mu, \sigma^2, p) = \frac{p(x_i\mid y; \mu,\sigma^2,p)\cdot p(y;\mu,\sigma^2,p)}{\sum_{z\in\{0,1\}^N} p(x_i\mid z;\mu,\sigma^2,p)\cdot p(z;\mu,\sigma^2,p)}
\]
with
\[
  p(x_i\mid z; \mu,\sigma^2,p) = \varphi\left(x_i;N_z\mu, N_z\sigma^2\right),\qquad p(z;\mu,\sigma^2,p) = p^{N_z}(1-p)^{N-N_z} = p^{N_z} - p^N.
\]
where $N_z$ denotes the number of $1$s in $z\in\{0,1\}^N$. In the M step, we have now have closed form solutions to all of the parameters. We have that
\begin{align*}
  \frac{\partial}{\partial\mu}\ell(\theta) = \sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\frac{-(x_i-N_z\mu)(-2N_z)}{2N_z\sigma^2} = \sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\frac{x_i-N_z\mu}{\sigma^2} = 0
\end{align*}
which gives a solution of
\[
  \mu = \frac{\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i(z)x_i}{\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i(z)N_z} = \boxed{\frac{\sum_{i=1}^n x_i}{\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)N_z}}
\]
for $\mu$,
\begin{align*}
  \frac{\partial}{\partial\sigma^2}\ell(\theta) &= \frac{\partial}{\partial\sigma^2}\sum_{i=1}^n\sum_{z\in\{0,1\}^N}Q_i^{(t)}(z)\left(-\frac{\log(2\pi N_z\sigma^2)}2 - \frac{(x_i-N_z\mu)^2}{2N_z\sigma^2}\right) \\
  &= \sum_{i=1}^n\sum_{z\in\{0,1\}^N}Q_i^{(t)}(z)\left(-\frac1{2\sigma^2} + \frac12\left(\frac{x_i-N_z\mu}{N_z\sigma^2}\right)^2\right) = 0
\end{align*}
which gives a solution of
\[
  \sigma^2 = \frac{\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\left(\frac{x_i-N_z\mu}{N_z}\right)^2}{\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)} = \boxed{\frac{\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\left(\frac{x_i-N_z\mu}{N_z}\right)^2}n}
\]
for $\sigma^2$, and
\begin{align*}
  \frac{\partial}{\partial p}\ell(\theta) &= \frac{\partial}{\partial p}\sum_{i=1}^n\sum_{z\in\{0,1\}^N}Q_i^{(t)}(z)\left(N_z\log p + (N-N_z)\log (1-p)\right) \\
  &= \sum_{i=1}^n\sum_{z\in\{0,1\}^N}Q_i^{(t)}(z)\left(\frac{N_z}p - \frac{N-N_z}{1-p}\right) = 0
\end{align*}
which gives a solution of
\[
  p = \frac{\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z) N_z}{\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)N} = \boxed{\frac{\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z) N_z}{Nn}}
\]
for $p$.

\subsubsection{Removing Zeros}
The above algorithms for EM in fact do not work, since they assign infinite probabilities to the $0$s when they are assigned to the all failure case. Thus, we must implement the algorithm on the nonzero values. In this case, the $z$ are drawn from $S := \{0,1\}^N\setminus \{(0,\dots,0)\}$ uniformly, and each $z\in S$ is drawn with probability
\[
  \frac1{1-\prod_{j=1}^N(1-p_j)}\prod_{j:z_j=1}p_j\prod_{j:z_j=0}(1-p_j).
\]
In the case of the binomial distribution model, the partial with respect to $p$ then turns out to be
\begin{align*}
  \frac{\partial}{\partial p_l}\ell(\theta) &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\log p_z \\
  &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\left(\sum_{j:z_j=1}\log p_j + \sum_{j:z_j=0}\log(1-p_j) - \log \left(1-\prod_{j=1}^N(1-p_j)\right)\right)
\end{align*}

\section{May 31, 2017}
\subsection{Progress}
\begin{itemize}
  \item start making visualizations for MOME estimators
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item get more data!
\end{itemize}

\section{June 1, 2017}
\subsection{Progress}
\begin{itemize}
  \item clear up problems on current approaches (lognormal distribution, lack of data)
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item explore other distributions for the postsynaptic response at each synaptic contact
  \item use lognormal distribution at each contact instead of normal distribution
\end{itemize}

\subsection{Meeting with Professor Barth and Professor Brasier}
The immediate goal at the moment is to do the first figures of the paper, e.g.\ the statement of the problem, why this small data set is enough to suspect that a simple binomial model will not suffice to explain the data.
\begin{itemize}
  \item failure rates of the trials, analysis with simple binomial model
  \item traces of the data
  \item aggregate results
\end{itemize}


\section{June 3, 2017}
\subsection{Progress}
\begin{itemize}
  \item found IGOR file parser \url{https://www.rdocumentation.org/packages/IgorR}, plotted wave files with it
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item view IGOR files on R
  \item ggplot histograms of the data, whole and first col
  \item failure rates of the columns (trials)
  \item plot failure rates of the columns as a function of the trials
  \item estimate probabilities of the highest amplitudes using binomial model, Markov, and Chebyshev
  \item awesome legends in the plots please (toggle with true/false options)
  \item plotting aggregate data
  \begin{itemize}
    \item other ways to represent the whole data set
  \end{itemize}
\end{itemize}

\subsection{Parsing IGOR files with R}
Use the \texttt{IgorR} package for reading the \texttt{pxp} files. Read the file with \texttt{read.pxp} and find the appropriate wave data. Then, convert to a \texttt{ts} object via \texttt{WaveToTimeSeries}. Finally, to plot with \texttt{ggplot2}, turn it into a data frame via \texttt{data.frame(Y = as.matrix(sweep1), t = time(sweep1))}.

\section{June 5, 2017}
\subsection{Progress}
\begin{itemize}
  \item ggplot histograms, first col and all
  \item plot failure rates, individually and aggregate
\end{itemize}

\section{June 7, 2017}
\subsection{Progress}
\begin{itemize}
  \item updated parameter estimates for the compound binomial-lognormal model
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item check model with other predictions, such as mean, variance, and failure rate
\end{itemize}


\subsection{Useful Properties of the Lognormal Distribution}
The cdf and quantile functions are given by
\[
  \begin{cases}
    F(x) = \Phi\left[\frac{\log(x)-\mu}\sigma\right] \\
    F^{-1}(p) = \exp\left[\mu + \sigma\Phi^{-1}(p)\right] & p\in(0,1)
  \end{cases}.
\]
The $t$th moment is given by
\[
  \mathbb E\left[X^k\right] = \exp\left[\mu t + \frac{\sigma^2t^2}2\right].
\]
In particular, the mean and variance are given by
\[
  \begin{cases}
    \mathbb E[X] = \exp\left[\mu + \frac{\sigma^2}2\right] \\
    \Var[X] = \exp\left[2(\mu+\sigma^2)\right] - \exp\left[2\mu+\sigma^2\right]
  \end{cases}.
\]

\subsection{Parameter Estimates for the Lognormal Model}
Under our new model that uses the lognormal distribution, the $p$ parameter estimate can stay, but we need to fix the estimates for $\mu$ and $\sigma$. Given $N$ and $p$, we have that
\[
  \begin{cases}
    \mathbb E[X] = \sum_{j=1}^N \mathbb E[Z_j] = N\exp\left[\mu + \frac{\sigma^2}2\right]p \\
    \Var[X] = \sum_{j=1}^N \Var[Z_j] = N\left(\exp\left[2(\mu+\sigma^2)\right] - \exp\left[2\mu+\sigma^2\right]\right)p
  \end{cases}.
\]
Now estimate $\mathbb E[X]$ by the sample mean $\overline X$ and $\Var[X]$ by the sample variance $S^2$. Then, after massaging, we get that the method of moments parameter estimates are
\[
  \begin{pmatrix}\mu\\\sigma\end{pmatrix} = \begin{pmatrix}-1&1\\2&-1\end{pmatrix}\begin{pmatrix}\frac12\log\left(\frac{S^2}{Np} + \left(\frac{\overline X}{Np}\right)^2\right)\\2\log\left(\frac{\overline X}{Np}\right)\end{pmatrix}.
\]

\section{June 8, 2017}
\subsection{Progress}
\begin{itemize}
  \item constructed bootstrap pivot confidence intervals for binomial parameters estimates
  \item plotted bootstrap results for each test
  \item plotted bootstrap results across tests
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item construct pivot confidence intervals for the binomial parameters
  \item plot bootstrap results for each parameter in a separate plot
  \item plot probability of large events (Markov, Chebyshev)
  \item add legends to testing plots (low priority)
\end{itemize}

\subsection{Estimating Probabilities of Large Amplitudes}
We use the Markov and Chebyshev inequalities to bound the tail probabilities. Markov's inequality gives us that
\[
  \mathbb P[X\geq t]\leq \frac{\mathbb E[X]}t
\]
and Chebyshev's inequality gives us that
\[
  \mathbb P[\abs{X-\mathbb E[X]}\geq t]\leq \frac{\Var[X]}{t^2}.
\]
Note that both
\[
  \mathbb E[X] = N\exp\left[\mu + \frac{\sigma^2}2\right]p
\]
and
\[
  \Var[X] = N\left(\exp\left[2(\mu+\sigma^2)\right] - \exp\left[2\mu+\sigma^2\right]\right)p = Npe^{2\mu+\sigma^2}\left(e^{\sigma^2}-1\right)
\]
are increasing functions of the parameters. Thus, we may find looser estimates that have more confidence by choosing the upper bound of the confidence intervals for the parameters.

\section{June 9, 2017}
\subsection{Progress}
\begin{itemize}
  \item plotted upper bounds for simulations
  \item plotted cumulative upper bounds
  \item plotted upper bounds for the max amplitude (Markov, Chebyshev)
  \item plotted upper bounds for the max amplitude (Monte Carlo)
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item plot for each type of theta (actual, estimated, upper bound)
  \item apply tests to actual data now
  \item approximation of probability of large amplitudes by simulation
\end{itemize}

\section{June 12, 2017}
\subsection{Progress}
\begin{itemize}
  \item plotted Monte Carlo results for just the first column
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item plot Monte Carlo results for just the first column
\end{itemize}

\section{June 13, 2017}
\subsection{Goals}
\begin{itemize}
  \item simulated histogram vs the original histogram
  \item finish figure 1
\end{itemize}

\section{June 14, 2017}
\subsection{Progress}
\begin{itemize}
  \item made slides and photoshop pictures
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item are high amplitudes correlated with each other for a single sweep?
  \item work on slides
  \item draw figures with photoshop
\end{itemize}

\section{June 15, 2017}
\subsection{Progress}
\begin{itemize}
  \item organize figure 1 ideas
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item figure 1
\end{itemize}

\subsection{Meeting with Professor Barth and Professor Brasier}
First figure should consist of something like:
\begin{enumerate}[a.]
  \item reconstructions
  \item multiple sweeps (maybe 10?) on top of each other
  \item heat map of amplitudes
  \item failure rates
  \item average failure rates
\end{enumerate}

\section{June 16, 2017}
\subsection{Progress}
\begin{itemize}
  \item plotted average failure rate plot (with and without ci)
  \item plotted heatmaps
  \item completed figure 1 first draft
\end{itemize}

\section{June 18, 2017}
\subsection{Goals}
\begin{itemize}
  \item fix average of failure rates to be a weighted average
  \item EM algorithm (monte carlo? direct computation of pdf?)
\end{itemize}

\section{June 20, 2017}
\subsection{Goals}
\begin{itemize}
  \item estimating pdfs and cdfs with the empirical distribution
\end{itemize}

\subsection{EM Algorithm for the Lognormal Binomial Model}
As before, we fix $N$, let $(x_i)_{i=1}^n$ be our observations of our model, let $y=(y_j)_{j=1}^N$ be our hidden observed values of the release success of each contact, and let $(\mu, \sigma^2, p)$ be the parameters of the lognormal binomial model. The log-likelihood of the parameters is given by
\[
  \ell(\theta) = \sum_{i=1}^n \log p(x_i;\theta) = \sum_{i=1}^n \log\sum_{y\in\{0,1\}^N} p(x_i\mid y;\theta)\cdot p(y;\theta).
\]
To be continued.

\section{June 21, 2017}
\subsection{Goals}
\begin{itemize}
  \item take log of data and back up lognormal data
  \item make good titles for figures
  \item white background in figures
\end{itemize}

\section{June 22, 2017}
\subsection{Progress}
\begin{itemize}
  \item revised figure 1
\end{itemize}

\section{June 25, 2017}
\subsection{Goals}
\begin{itemize}
  \item work on figure 2
\end{itemize}

\section{June 28, 2017}
\subsection{Progress}
\begin{itemize}
  \item design object for analysis
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item analysis of 17march2016g
\end{itemize}

\section{June 30, 2017}
\subsection{Progress}
\begin{itemize}
  \item complete software for igor analysis
\end{itemize}

\section{July 1, 2017}
\subsection{Progress}
\begin{itemize}
  \item complete igor analysis of 17march2016g.pxp
\end{itemize}

\section{July 3, 2017}
\subsection{Progress}
\begin{itemize}
  \item complete new version of figure 1 with 17march2016g.pxp
\end{itemize}

\section{July 4, 2017}
\subsection{Progress}
\begin{itemize}
  \item complete new version of figure 1 with 17march2016g.pxp
\end{itemize}

\subsection{Latex for Figures}

\[
	\sum_{i=1}^N \binom{N}{i} (1-p)^{N-i} p^i = ((1-p) + p)^N = 1
\]

\begin{center}
  \begin{tabular}{c c c}
  	\# Simultaneous Contacts & Amplitude (mV) & Probability \\
    \hline
    $0$ & $0.69\cdot 0 = 0.00$ & $\binom{5}{0}(1-0.11)^5\cdot 0.11^0\approx 0.55841$ \\[4pt]
    $1$ & $0.69\cdot 1 = 0.69$ & $\binom{5}{1}(1-0.11)^4\cdot 0.11^1\approx 0.34508$ \\[4pt]
    $2$ & $0.69\cdot 2 = 1.38$ & $\binom{5}{2}(1-0.11)^3\cdot 0.11^2\approx 0.08530$ \\[4pt]
    $3$ & $0.69\cdot 3 = 2.07$ & $\binom{5}{3}(1-0.11)^2\cdot 0.11^3\approx 0.01054$ \\[4pt]
    $4$ & $0.69\cdot 4 = 2.76$ & $\binom{5}{4}(1-0.11)^1\cdot 0.11^4\approx 0.00065$ \\[4pt]
    $5$ & $0.69\cdot 5 = 3.45$ & $\binom{5}{5}(1-0.11)^0\cdot 0.11^5\approx 0.00002$ \\
    \hline
  \end{tabular}
\end{center}

\section{July 6, 2017}
\subsection{Progress}
\begin{itemize}
  \item complete figures 1, 2, 3
\end{itemize}

\bibliography{citations}
\bibliographystyle{plain}

\end{document}
