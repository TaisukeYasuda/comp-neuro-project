\documentclass{article}
\usepackage{tai}

\title{Research Notebook}
\author{Taisuke Yasuda}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{May 20, 2017}
\subsection{Progress}
\begin{itemize}
  \item set up github for project
  \item plotted histograms of first trials
  \item plotted scatter plots of first trials (for stationarity)
\end{itemize}

\subsection{Statistical Model}
Recall that our model of the process is
\[
  X = \sum_{j=1}^N Z_j,\qquad Y_j = \Bernoulli(p_j),\qquad (Z_j\mid Y_j = 1)\sim N(\mu_j,\sigma_j^2), \qquad (Z_j\mid Y_j = 0) = 0
\]
where $N, \mu_j, \sigma_j^2, p_j$ are all unknown parameters of the model. The $Z_j$ random variable models the response amplitude of a single contact of which there are $N$, and the $Y_j$ random variable models the release success of a single contact. We assume that all of the $Z_j$ and the $Y_j$ are independent.

\subsubsection{Justification}
The additivity of the potential is justified by Petterson and Einevoll \cite{pettersen2008amplitude}. The use of a Gaussian distribution for individual contacts is justified by Magee and Cook \cite{magee2000somatic}.

\section{May 22, 2017}
\subsection{Progress}
\begin{itemize}
  \item derived point estimates for release probability, mean response amplitude, and response amplitude variance under the constant parameter models
\end{itemize}

\subsection{Point Estimation of Model Parameters}
\subsubsection{Estimations Under a Simplified Model}
We have that the expectation of the model is
\[
  \mathbb E[X] = \sum_{j=1}^N \mathbb E[Z_j] = \sum_{j=1}^N \mu_j\cdot p_j
\]
and that the variance is
\[
  \Var[X] = \sum_{j=1}^N \Var[Z_j] = \sum_{j=1}^N \sigma_j^2\cdot p_j.
\]
Also note that the failure rate of the model, i.e.\ the probability that all $N$ contacts fail to release, can be approximated by
\[
  \mathbb P[X = 0]\approx \prod_{j=1}^N (1-p_j)
\]
by assuming that a contact produces a positive response everytime it succeeds in releasing a vesicle. Thus, with simplifying assumptions that all the $\mu_j$, $\sigma_j^2$, and $p_j$ are the same across the $N$ points of contact, and by fixing a value of $N$, we may find a plugin estimator for $\hat p$ and method of moments estimators for $\hat\mu$ and $\hat\sigma^2$ that depend on $\hat p$. If we let $\overline X$ be the sample mean, $S^2$ be the sample variance, and $p_f$ be the sample failure rate, these estimates are given by
\[
  \hat p = 1 - \sqrt[N]{p_f}, \qquad \hat\mu = \frac{\overline X}{N\hat p}, \qquad \hat\sigma^2 = \frac{S^2}{N\hat p}.
\]

\subsubsection{EM Algorithm}
Let us return to the general case. Suppose that we treat the response amplitudes of the individual contacts, the $Z_j$ from $1\leq j\leq N$, as latent variables. Then, the joint distribution of the latent and observed variables will be an exponential family, so EM algorithm should work very well.

\section{May 24, 2017}
\subsection{Goals}
\begin{itemize}
  \item how was the data collected? can we really justify our model?
  \item workout details of EM and implement
\end{itemize}

\subsection{Progress}
\begin{itemize}
  \item derive E step of the EM algorithm
\end{itemize}

\subsection{EM Algorithm}
We refer to lecture notes by Andrew Ng \cite{ng2016em} for the EM reference. Suppose that we fix $N$ and let $(x_i)_{i=1}^n$ be our observations of our model, let $y = (y_j)_{j=1}^N$ be the hidden observed values of the release success of each individual contact, and let $(\mu, \sigma^2, p) = (\mu_j,\sigma_j^2,p_j)_{j=1}^N$. Then, the log-likelihood of the parameters is given by
\begin{align*}
  \ell\left(\mu, \sigma^2, p\right) &= \sum_{i=1}^n\log p(x_i; \mu, \sigma^2, p) = \sum_{i=1}^n\log \sum_{y\in\{0,1\}^N} p(x_i\mid y;\mu,\sigma^2,p)\cdot p(y;\mu,\sigma^2,p).
\end{align*}
In the above, the sum ranges over all possible assignments of the release success of the $N$ individual contacts. Note that $p(z_i\mid y;\mu,\sigma^2,p)$ is simply the pdf of the sum of Gaussian variables which also takes a Gaussian distribution, and $p(y;\mu,\sigma^2,p)$ is simply given by the product of $p_j$ or $1-p_j$ for each $1\leq j\leq N$, as appropriate. Also note that this problem is very similar to a Gaussian mixture model, where we sample from $2^N$ different Gaussian distributions where each Gaussian is a sum of the original $N$ Gaussians. The difference is that the Gaussians that we find may not be arbitrary, but in fact must be generated by $N$ Gaussians in a specific way.

\subsubsection{E step}

In the E step, we find a probability distribution over the release success of the individual contacts, given the observed amplitude $x_i$. Denote $\varphi(x;\mu,\sigma^2)$ as the pdf of a Gaussian with mean $\mu$ and variance $\sigma^2$ evaluated at the point $x$. Then,
\[
  Q_i(y) = p(y\mid x_i; \mu, \sigma^2, p) = \frac{p(x_i\mid y; \mu,\sigma^2,p)\cdot p(y;\mu,\sigma^2,p)}{\sum_{z\in\{0,1\}^N} p(x_i\mid z;\mu,\sigma^2,p)\cdot p(z;\mu,\sigma^2,p)}
\]
where for all assignments of release successes $z\in\{0,1\}^N$,
\[
  p(x_i\mid z; \mu,\sigma^2,p) = \varphi\left(x_i;\sum_{j:z_j=1} \mu_j, \sum_{j:z_j=1} \sigma_j^2\right),\qquad p(z;\mu,\sigma^2,p) = \prod_{j:z_j=1}p_j\prod_{j:z_j=0}(1-p_j).
\]

\section{May 27, 2017}
\subsection{Progress}
\begin{itemize}
  \item derive M step of the EM algorithm for $p_l$
\end{itemize}

\subsection{EM Algorithm (cont.)}

\subsubsection{M step}

In the M step, we maximize the function
\[
  \ell(\theta) = \sum_{i=1}^n \sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\log \frac{p(x_i, z;\theta)}{Q_i^{(t)}(z)}
\]
and set the value of $\theta$ that maximizes the above to the new estimate for $\theta$, where $\theta$ is our $\mu,\sigma^2,p$. For notational convenience, let $\mu_z := \sum_{j:z_j=1}\mu_j$, $\sigma_z^2 := \sum_{j:z_j=1} \sigma_j^2$, and $p_z := \prod_{j:z_j=1}p_j\prod_{j:z_j=0}(1-p_j)$.
\begin{itemize}
  \item \textbf{Maximization with respect to $\mu_l$}

  We have that
  \begin{align*}
    \frac{\partial}{\partial\mu_l}\ell(\theta) &= \frac{\partial}{\partial\mu_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\log \frac{p(x_i, z;\theta)}{Q_i^{(t)}(z)}
    = \frac{\partial}{\partial\mu_l}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\log p(x_i, z;\theta) - \log Q_i^{(t)}(z)\right) \\
    &= \frac{\partial}{\partial\mu_l}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\log \left(\frac1{\sqrt{2\pi\sigma_z^2}}e^{-\frac{(x_i-\mu_z)^2}{2\sigma_z^2}}\cdot p_z\right) - \log Q_i^{(t)}(z)\right) \\
    &= \frac{\partial}{\partial\mu_l}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\log \frac1{\sqrt{2\pi\sigma_z^2}} -\frac{(x_i-\mu_z)^2}{2\sigma_z^2} + \log p_z - \log Q_i^{(t)}(z)\right) \\
    &= -\frac{\partial}{\partial\mu_l}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\frac{(x_i-\mu_z)^2}{2\sigma_z^2}
    = -\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\frac{-2(x_i-\mu_z)}{2\sigma_z^2} \\
    &= \sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\frac{x_i-\mu_z}{\sigma_z^2}.
  \end{align*}

  We cannot isolate $\mu_l$, so we need to solve with the other $\mu_j$ as well as the $\sigma_j^2$.

  \item \textbf{Maximization with respect to $\sigma_l^2$}

  We have that
  \begin{align*}
    \frac{\partial}{\partial\sigma_l^2}\ell(\theta) &= \frac{\partial}{\partial\sigma_l^2}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\log \frac{p(x_i, z;\theta)}{Q_i^{(t)}(z)} \\
    &= \frac{\partial}{\partial\sigma_l^2}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\log \frac1{\sqrt{2\pi\sigma_z^2}} -\frac{(x_i-\mu_z)^2}{2\sigma_z^2} + \log p_z - \log Q_i^{(t)}(z)\right) \\
    &= \frac{\partial}{\partial\sigma_l^2}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\log\frac1{\sqrt{2\pi\sigma_z^2}} - \frac{(x_i-\mu_z)^2}{2\sigma_z^2}\right) \\
    &= \frac{\partial}{\partial\sigma_l^2}\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(-\frac{\log(2\pi\sigma_z^2)}2 - \frac{(x_i-\mu_z)^2}{2\sigma_z^2}\right)
    = \frac12\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\left(\frac{x_i-\mu_z}{\sigma_z^2}\right)^2-\frac1{\sigma_z^2}\right).
  \end{align*}

  Now we need to solve for all the $\mu_j$ and $\sigma_j$ when the above is set to $0$. It is worth noting that the $Z$-score with respect to $N(\mu_z,\sigma_z^2)$ appears in both of the derivatives for $\mu_l$ and $\sigma_l^2$.

  \item \textbf{Maximization with respect to $p_l$}

  As before, we have that
  \begin{align*}
    \frac{\partial}{\partial p_l}\ell(\theta) &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\log \frac{p(x_i, z;\theta)}{Q_i^{(t)}(z)} \\
    &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\left(\log \frac1{\sqrt{2\pi\sigma_z^2}} -\frac{(x_i-\mu_z)^2}{2\sigma_z^2} + \log p_z - \log Q_i^{(t)}(z)\right) \\
    &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\log p_z \\
    &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\sum_{z\in\{0,1\}^N} Q_i^{(t)}(z)\left(\sum_{j:z_j=1}\log p_j + \sum_{j:z_j=0}\log(1-p_j)\right) \\
    &= \frac{\partial}{\partial p_l}\sum_{i=1}^n\left(\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\log p_l + \sum_{\substack{z\in\{0,1\}^N \\ z_l = 0}} Q_i^{(t)}(z)\log(1-p_l)\right) \\
    &= \sum_{i=1}^n\left(\frac{\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}}Q_i^{(t)}(z)}{p_l} - \frac{\sum_{\substack{z\in\{0,1\}^N \\ z_l = 0}}Q_i^{(t)}(z)}{1-p_l}\right).
  \end{align*}
  Setting this to $0$ gives us that
  \[
    0 = \sum_{i=1}^n\left(\frac{\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}}Q_i^{(t)}(z)}{p_l} - \frac{\sum_{\substack{z\in\{0,1\}^N \\ z_l = 0}}Q_i^{(t)}(z)}{1-p_l}\right),
  \]
  which solves to
  \[
    p_l = \frac{\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}}Q_i^{(t)}(z)}{\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}}Q_i^{(t)}(z) + \sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 0}}Q_i^{(t)}(z)} = \boxed{\frac{\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}}Q_i^{(t)}(z)}n}.
  \]
\end{itemize}

\section{May 28, 2017}

\subsection{Progress}
\begin{itemize}
  \item gave up on a closed form for the M step, decided on gradient descent for the EM algorithm
\end{itemize}

\subsection{EM Algorithm (cont.)}
\subsubsection{M step (cont.)}
Since we cannot solve for the optimal $\mu_l$ and $\sigma_l^2$ in closed form, we opt for numerically determining the $\mu_l$ and $\sigma_l^2$ via gradient descent \cite{bishop2006pattern}. Recall that we have already computed the gradient:
\[
  \begin{cases}
    \frac{\partial}{\partial\mu_l}\ell(\theta) = \sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\frac{x_i-\mu_z}{\sigma_z^2} \\
    \frac{\partial}{\partial\sigma_l^2}\ell(\theta) = \frac12\sum_{i=1}^n\sum_{\substack{z\in\{0,1\}^N \\ z_l = 1}} Q_i^{(t)}(z)\left(\left(\frac{x_i-\mu_z}{\sigma_z^2}\right)^2-\frac1{\sigma_z^2}\right)
  \end{cases}.
\]
Now, it is straightforward to implement the algorithm.

\subsection{Parametric Bootstrap}
Once we have an estimator for the parameters of the model, we may use parametric bootstrap to estimate the variance of the estimator, as explained in the classical text by Casella and Berger \cite{casella2002statistical}. Using the parameters $\hat\theta$ estimated by the EM algorithm, we may simulate $n$ values $X_1^*, \dots, X_n^*\sim f(x;\hat\theta)$ and approximate the MLE using the EM algorithm $B$ times, and use the sample variance of those $B$ estimates of the MLE as the variance of the estimator.

\section{May 29, 2017}
\subsection{Progress}
\begin{itemize}
  \item implement EM algorithm upto expectation step
\end{itemize}

\subsection{Goals}
\begin{itemize}
  \item validate $p_j$ estimates with the sample failure rate
  \item validate $\mu_j$ estimates with the sample mean
  \item validate $\sigma_j^2$ estimates with the sample variance
  \item after writing slower version of EM algorithm, write a faster version
\end{itemize}

\bibliography{citations}
\bibliographystyle{plain}

\end{document}
